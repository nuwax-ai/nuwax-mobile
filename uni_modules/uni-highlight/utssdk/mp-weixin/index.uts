import * as vsctm from '../../common/uni-textmate/main';
import onig from './onig/main.js';
// import javascriptGrammar from '../../static/grammar/javascript.tmLanguage.json';


export interface IToken {
	readonly startIndex: number;
	readonly endIndex: number;
	readonly scopes: string[];
}
export interface ILineTokens{
	readonly tokens:IToken[],
	state:any
}

export type HighLighterOptions = {
	languages: Record<string, any>
}

export type CreateHighLighterRes = {
	tokenizeFullText(langId: string, fullCodeText:string): Promise<ILineTokens[]>
	tokenizeLine(langId: string, lineText: string, state: any): Promise<ILineTokens | null>
}

async function loadOnigWasmFile(): Promise<ArrayBuffer> {
	let fileManager = uni.getFileSystemManager()
	let content = fileManager.readFileSync('/uni_modules/uni-highlight/static/_onig.wasm')
	return content
	
}

export async function createHighLighter(options: HighLighterOptions):Promise<CreateHighLighterRes> {
	const languages = options.languages;
	const langMap: Map<string, string> = new Map();
	const scopeMap: Map<string, object> = new Map();
	
	for(let id in languages){
		langMap.set(id, languages[id]?.scopeName)
		scopeMap.set(languages[id].scopeName, languages[id])
	}
	
	// let wasmFile = await loadOnigWasmFile()
	
	const vscodeOnigurumaLib = onig.loadWASM({path: '/uni_modules/uni-highlight/static/_onig.wasm'}).then(() => {
		return {
			createOnigScanner(patterns) { return new onig.OnigScanner(patterns); },
			createOnigString(s) { return new onig.OnigString(s); }
		};
	});
	const registry = new vsctm.Registry({
		onigLib: vscodeOnigurumaLib,
		loadGrammar: (scopeName) => {
			try {
				let content = JSON.stringify(scopeMap.get(scopeName))
				const filePath = "/static/javascript.tmLanguage.json";
				return Promise.resolve(vsctm.parseRawGrammar(content, filePath));
			} catch {
				return Promise.reject(null)
			}
			
			
		}
	});
	
	return {
		async tokenizeFullText(langId: string, fullCodeText:string): Promise<ILineTokens[]> {
			let scopeName = langMap.get(langId)
			console.log(111)
			
			if (!scopeName) return []
			const grammar = await registry.loadGrammar(scopeName)
			const text = fullCodeText.split(/\r\n|\r|\n/);
			let ruleStack = vsctm.INITIAL;
			let res: ILineTokens[] = []
			for (let i = 0; i < text.length; i++) {
				const line = text[i] + "\n";
				const lineTokens = grammar?.tokenizeLine(line, ruleStack);
				if (!lineTokens) return []
				const tokens:IToken[] = []
				for (let j = 0; j < lineTokens.tokens.length; j++) {
					const token = lineTokens.tokens[j];
					const iToken: IToken = {
						startIndex: token.startIndex,
						endIndex: token.endIndex,
						scopes: token.scopes
					}
					tokens.push(iToken)
				}
				res.push({
					tokens,
					state: lineTokens.ruleStack
				})
				ruleStack = lineTokens.ruleStack;
			}
			
			return res
		},
		async tokenizeLine(langId: string, lineText: string, state: any): Promise<ILineTokens | undefined> {
			let scopeName = langMap.get(langId)
			if (!scopeName) return
			let ruleStack = state ? state : vsctm.INITIAL;
			const grammar = await registry.loadGrammar(scopeName)
			const lineTokens = grammar?.tokenizeLine(lineText, ruleStack);
			if (!lineTokens) return
			const tokens:IToken[] = []
			for (let j = 0; j < lineTokens.tokens.length; j++) {
				const token = lineTokens.tokens[j];
				const iToken: IToken = {
					startIndex: token.startIndex,
					endIndex: token.endIndex,
					scopes: token.scopes
				}
				tokens.push(iToken)
			}
			return {
				tokens,
				state: lineTokens.ruleStack
			}
		}
	}
}